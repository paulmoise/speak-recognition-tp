{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Speaker verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 : Installation du toolkit Kiwano\n",
    "\n",
    "Kiwano est une boîte à outils open source pour la vérification des locuteurs basée sur PyTorch. L'objectif de ce TP est d'implémenter chaîne basique de vérification du locuteur en passant par les étapes suivantes : extraction de filtres de banques de fréquences,  normalisation CMVN, extraction d’embeddings du locuteur et calcul de la similarité avec une mesure du cosinus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/mrouvier/kiwano/\n",
    "! cd kiwano/ && pwd\n",
    "! cd kiwano/ && pip install -r requirements.txt\n",
    "! cd kiwano/ && pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Extraction des filter banks\n",
    "\n",
    "### Objectif\n",
    "L'objectif est d'analyser la manière dont les caractéristiques acoustiques d'un signal audio sont représentées à travers des coefficients issus des filter banks. Ce code permet de charger un fichier audio (ici enrollment.wav) et d'en extraire les filter banks.\n",
    "\n",
    "### Questions\n",
    "- A partir des filter banks, pouvez vous donnez la durée du fichier audio ?\n",
    "- Quel est le nombre de coefficient extrait ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from IPython.display import Audio \n",
    "import kiwano\n",
    "from kiwano.features import Fbank\n",
    "\n",
    "#Initialisation de l'extracteur de features (Filter Bank)\n",
    "feature_extractor = Fbank()\n",
    "\n",
    "#Chargement d'un fichier audio\n",
    "wav, sr = torchaudio.load(\"enrollment.wav\")\n",
    "\n",
    "#Lecture et affichage du fichier audio directement dans le notebook\n",
    "display(Audio(\"enrollment.wav\", autoplay=True))\n",
    "\n",
    "#Extraction des features\n",
    "f = feature_extractor.extract(wav, sampling_rate=sr)\n",
    "\n",
    "#Affichage\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3 : Normalisation CMVN\n",
    "\n",
    "### Questions\n",
    "- Que fait la normalisation CMVN dans le code ?\n",
    "- Pourquoi est-il nécessaire de réaliser cette normalisation ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import kiwano\n",
    "from kiwano.augmentation import CMVN\n",
    "\n",
    "#Initialisation de la normalisation CMVN\n",
    "cmvn = CMVN()\n",
    "\n",
    "#Initialisation de l'extracteur de features (Filter Bank)\n",
    "feature_extractor = Fbank()\n",
    "\n",
    "#Chargement d'un fichier audio\n",
    "wav, sr = torchaudio.load(\"enrollment.wav\")\n",
    "\n",
    "#Extraction des features\n",
    "f = feature_extractor.extract(wav, sampling_rate=sr)\n",
    "\n",
    "#Affichage de la matrice\n",
    "print(f)\n",
    "\n",
    "#Application de la normalisation CMVN\n",
    "f_cmvn = cmvn(f)\n",
    "\n",
    "#Affichage de la matrice\n",
    "print(f_cmvn)\n",
    "\n",
    "m = torch.mean(f, dim=0)\n",
    "\n",
    "#Affichage de la matrice\n",
    "print(f-m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4 : Extraction d'un speaker embedding\n",
    "\n",
    "### Objectif\n",
    "L'objectif de cette étape est d'extraire les embeddings du locuteur à l'aide d'un modèle pré-entraîné. Le code actuel permet d'extraire un embedding à partir du fichier audio \"enrollment.wav\".\n",
    "\n",
    "### Exercice\n",
    "- Extraire un embedding à partir du fichier \"test1.wav\".\n",
    "- De quelle taille est l'embedding ?\n",
    "- Comparer les embeddings issue du fichier \"enrollment.wav\" et \"test1.wav\". La comparaison se fait en calculant un score de similarité à l'aide de la distance cosinus (cosine similarity).\n",
    "\n",
    "- Extraire un embedding à partir du fichier \"enrollment.wav\".\n",
    "- Comparer les embeddings issue du fichier \"enrollment.wav\" et \"test2.wav\". La comparaison se fait en calculant un score de similarité à l'aide de la distance cosinus (cosine similarity).\n",
    "\n",
    "- Ecouter les fichiers audio \"enrollment.wav\", \"test1.wav\" et \"test2.wav\". En fonction de ce que vous entendez, quel locuteur semble le plus proche de \"enrollment.wav\" ? Que révèlent les scores de similarité calculés à propos des ressemblances entre les locuteurs ?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kiwano\n",
    "from kiwano.features import Fbank\n",
    "from kiwano.augmentation import CMVN\n",
    "from kiwano.model import ResNetV2\n",
    "\n",
    "#Lecture et affichage du fichier audio directement dans le notebook\n",
    "display(Audio(\"enrollment.wav\", autoplay=True))\n",
    "display(Audio(\"test1.wav\", autoplay=True))\n",
    "display(Audio(\"test2.wav\", autoplay=True))\n",
    "\n",
    "#Initialisation de la normalisation CMVN et de l'extracteur de features\n",
    "cmvn = CMVN()\n",
    "feature_extractor = Fbank()\n",
    "\n",
    "#Creation du modèle ResNet et initialisation des poids\n",
    "resnet_model = ResNetV2(num_classes=21000)\n",
    "resnet_model.load_state_dict(torch.load(\"model/model.ckpt\", map_location=\"cpu\")[\"model\"])\n",
    "resnet_model.eval()\n",
    "\n",
    "#Chargement d'un fichier audio\n",
    "wav, sr = torchaudio.load(\"enrollment.wav\")\n",
    "\n",
    "##Extraction des features et application de la normalisation CMVN\n",
    "f = cmvn(\n",
    "    feature_extractor.extract(wav, sampling_rate=sr)\n",
    ")\n",
    "\n",
    "#Extraction de l'embedding\n",
    "f = f.unsqueeze(0).unsqueeze(1)\n",
    "emb = resnet_model(f)\n",
    "\n",
    "print(emb[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5 : Bruit blanc\n",
    "\n",
    "Le bruit blanc est un signal sonore qui comprend toutes les fréquences audibles (ou toutes les valeurs possibles dans un domaine donné), avec une intensité ou puissance uniforme pour chaque fréquence. Il se manifeste par un son constant, similaire à celui d'une télévision ou d'une radio mal réglée. Le terme \"blanc\" est employé par analogie avec la lumière blanche, qui contient toutes les longueurs d'onde du spectre visible. Dans cet exercice, nous allons appliquer du bruit blanc à chaque fichier audio et observer l'impact que cela a sur le modèle.\n",
    "\n",
    "### Exercice\n",
    "- Répondez aux questions de l'étape 4, mais cette fois en ajoutant du bruit blanc à chaque fichier audio.\n",
    "- Pourquoi la distance entre les fichiers diffère-t-elle ? Quelle conclusion peut-on en tirer ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kiwano\n",
    "from kiwano.features import Fbank\n",
    "from kiwano.augmentation import CMVN\n",
    "from kiwano.model import ResNetV2\n",
    "import IPython.display as ipd\n",
    "\n",
    "def white_noise(waveform):\n",
    "    noise_level = 0.01\n",
    "    white_noise = torch.randn(waveform.shape) * noise_level\n",
    "    waveform_with_noise = waveform + white_noise\n",
    "    waveform_with_noise = torch.clamp(waveform_with_noise, -1.0, 1.0)\n",
    "    return waveform_with_noise\n",
    "\n",
    "\n",
    "cmvn = CMVN()\n",
    "feature_extractor = Fbank()\n",
    "wav, sr = torchaudio.load(\"enrollment.wav\")\n",
    "wav_with_noise = white_noise(wav)\n",
    "\n",
    "display(ipd.Audio(wav, rate=sr, autoplay=True))\n",
    "display(ipd.Audio(wav_with_noise, rate=sr, autoplay=True))\n",
    "\n",
    "resnet_model = ResNetV2(num_classes=21000)\n",
    "resnet_model.load_state_dict(torch.load(\"model/model.ckpt\", map_location=\"cpu\")[\"model\"])\n",
    "resnet_model.eval()\n",
    "\n",
    "f = cmvn(\n",
    "    feature_extractor.extract(wav_with_noise, sampling_rate=sr)\n",
    ")\n",
    "\n",
    "\n",
    "f = f.unsqueeze(0).unsqueeze(1)\n",
    "emb = resnet_model(f)\n",
    "print(emb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 6 : Calculer l'EER\n",
    "\n",
    "### Objectif\n",
    "L'objectif de cette étape est de calculer l'EER (Equal Error Rate) en utilisant un fichier de \"trials\". L'EER est une mesure utilisée pour évaluer les performances d'un système de reconnaissance vocale ou d'authentification de locuteur. Il représente le point où le taux de fausses acceptations (False Acceptance Rate, FAR) est égal au taux de faux rejets (False Rejection Rate, FRR).\n",
    "\n",
    "Dans le fichier \"trials.txt\", chaque ligne contient trois colonnes :\n",
    "- Première colonne : fichier d'enrollment (enregistrement de référence du locuteur).\n",
    "- Deuxième colonne : fichier de test (enregistrement à comparer).\n",
    "- Troisième colonne : indication si les deux fichiers appartiennent au même locuteur (target, noté par 1) ou à des locuteurs différents (non-target, noté par 0).\n",
    "\n",
    "\n",
    "### Exercice\n",
    "- Calculer l'EER du modèle en utilisant les paires d'enregistrement dans le fichier \"trials.txt\".\n",
    "- Dessinez la courbe DET du système sur ce corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichier trials.txt\n",
    "\n",
    "Quelques lignes du fichier trials.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head trials.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer EER\n",
    "\n",
    "Exemple pour calculer un EER. L'EER prend en entrée labels et scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "labels = [0, 0, 0, 1, 1, 1, 1]\n",
    "scores = [0.1, 0.2, 0.3, 0.4, 0.5, 0.1, 0.6]\n",
    "\n",
    "fpr, tpr, threshold = sklearn.metrics.roc_curve(labels, scores, pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "\n",
    "eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "eer = (eer_1 + eer_2) / 2\n",
    "\n",
    "print(eer*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 7 : Normaliser les speaker embeddings\n",
    "\n",
    "## Objectif\n",
    "L'objectif de cette étape est de normaliser les *speaker embeddings* en les centrant par rapport à une moyenne calculée à partir des fichiers audio présents dans le répertoire train/. Cette normalisation permet de réduire les informations non pertinentes et d'obtenir une meilleure représentation des locuteurs.\n",
    "\n",
    "### Exercice\n",
    "- **Calcul des moyennes des speaker embeddings** : Calculez la moyenne des *speaker embeddings* issus des fichiers audio du répertoire train/. Cette moyenne servira de référence pour centrer les représentations des locuteurs.\n",
    "- **Normalisation des embeddings** : Normalisez les *speaker embeddings* des enregistrements présents dans le fichier *trials.txt* en soustrayant cette moyenne calculée.\n",
    "- **Recalculez l'EER en utilisant les speaker embeddings normalisés** : Recalculez l'EER en utilisant les *speaker embeddings* normalisés sur les paires d'enregistrement présentes dans le fichier *trials.txt*. Quels changements observez-vous après la normalisation ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 8 : ResNet\n",
    "\n",
    "### Question\n",
    "- En regardant le code source de Kiwano, donnez l'architecture du ResNet ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 9 : Voice Activity Detection (exercice optionnel)\n",
    "\n",
    "### Objectif\n",
    "Pour ceux qui ont terminé, certains fichiers audio peuvent contenir des zones de silence. Ces silences peuvent affecter la qualité des embeddings. L'objectif de cet exercice est de supprimer ces zones de silence dans chaque fichier audio en utilisant l'outil SILERO : https://github.com/snakers4/silero-vad\n",
    "\n",
    "### Exercice\n",
    "- Ouvrir un fichier audio.\n",
    "- Détecter les zones de silence à l'aide de Silero VAD.\n",
    "- Supprimer ces zones de silence.\n",
    "- Calculer l'embedding à partir du fichier audio nettoyé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "\n",
    "model = load_silero_vad()\n",
    "wav = read_audio(\"enrollment.wav\")\n",
    "speech_timestamps = get_speech_timestamps(wav, model)\n",
    "\n",
    "wav_witout_silence = collect_chunks(speech_timestamps, wav)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
